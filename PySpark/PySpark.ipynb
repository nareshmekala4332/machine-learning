{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark\n",
    " \n",
    " * Python supports Object-oreinted, Array based, Asychonous based and Function based programming.\n",
    " * To deal with Big Data concepts in Python, We use pySpark library.\n",
    " * PySpark uses functional programing which allows to distribute functions in clustering esily. \n",
    " * No need to mainttain extrnal state like Global as functional always returns result.\n",
    " * Supports anonymous functions-> Lamda fuunctions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'awesome', 'is', 'programming']\n"
     ]
    }
   ],
   "source": [
    "X=['Python','is','awesome','programming']\n",
    "\n",
    "print(sorted(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * lambda functions in Python are defined inline and are limited to a single expression.\n",
    " * The key parameter to sorted is called for each item in the iterable. This makes the sorting case-insensitive by changing all the strings to lowercase before the sorting takes place.\n",
    "\n",
    "* This is a common use-case for lambda functions, small anonymous functions that maintain no external state.\n",
    "\n",
    "* Other common functional programming functions exist in Python as well, such as filter(), map(), and reduce(). All these functions can make use of lambda functions or standard functions defined with def in a similar manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awesome', 'is', 'programming', 'Python']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(X,key= lambda ar:ar.lower())) # lambda functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter(), map(), and reduce()\n",
    "* The built-in filter(), map(), and reduce() functions are all common in functional programming. You’ll soon see that these concepts can make up a significant portion of the functionality of a PySpark program.\n",
    "\n",
    "* It’s important to understand these functions in a core Python context. Then, you’ll be able to translate that knowledge into PySpark programs and the Spark API.\n",
    "\n",
    "* filter() filters items out of an iterable based on a condition, typically expressed as a lambda function:\n",
    "* filter() takes an iterable, calls the lambda function on each item, and returns the items where the lambda returned True.\n",
    "* Calling list() is required because filter() is also an iterable. filter() only gives you the values as you loop over them. list() forces all the items into memory at once instead of having to use a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 13]\n"
     ]
    }
   ],
   "source": [
    "seq = [0, 1, 2, 3, 5, 8, 13] \n",
    "  \n",
    "# result contains odd numbers of the list \n",
    "result = filter(lambda x: x % 2, seq) \n",
    "\n",
    "print(list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* map() is similar to filter() in that it applies a function to each item in an iterable, but it always produces a 1-to-1 mapping of the original items. The new iterable that map() returns will always have the same number of elements as the original iterable, which was not the case with filter():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PYTHON', 'IS', 'AWESOME', 'PROGRAMMING']\n"
     ]
    }
   ],
   "source": [
    "print(list(map(lambda ar: ar.upper(),X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', 5)]\n"
     ]
    }
   ],
   "source": [
    "my_strings = ['a', 'b', 'c', 'd', 'e']\n",
    "my_numbers = [1,2,3,4,5]\n",
    "\n",
    "results = list(map(lambda x, y: (x, y), my_strings, my_numbers))\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the function being applied can be a standard Python function created with the def keyword or a lambda function.\n",
    "\n",
    "* However, reduce() doesn’t return a new iterable. Instead, reduce() uses the function called to reduce the iterable to a single value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "l=[1,2,3,4,5,6,7,8,9,0]\n",
    "print(reduce(lambda x,y: x+y,l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sets are another common piece of functionality that exist in standard Python and is widely useful in Big Data processing. Sets are very similar to lists except they do not have any ordering and cannot contain duplicate values. You can think of a set as similar to the keys in a Python dict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Install pyspark : pip install pyspark\n",
    "\n",
    "* PySpark program isn’t that much different from a regular Python program, but the execution model can be very different from a regular Python program, especially if you’re running on a cluster.\n",
    "\n",
    "* There can be a lot of things happening behind the scenes that distribute the processing across multiple nodes if you’re on a cluster. However, for now, think of the program as a Python program that uses the PySpark library.\n",
    "\n",
    "* We will take one sample problem statement . Read a file and print words count\n",
    "\n",
    "* Now that you’ve seen some common functional concepts that exist in Python as well as a simple PySpark program, it’s time to dive deeper into Spark and PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words count ::  8\n",
      "PythonRDD[3] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "#Create a SparkContext using every core of the local machine\n",
    "sc= pyspark.SparkContext(\"local[*]\")\n",
    "\n",
    "# loda each line of the data file into RDD\n",
    "rdd = sc.textFile(\"E:\\SparkScala\\pyspark.txt\")\n",
    "\n",
    "print(\"Total words count :: \", rdd.count())\n",
    "\n",
    "\n",
    "print(rdd.filter(lambda line: \"python\" in line.lower()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "python_lines = rdd.filter(lambda line: \"python\" in line.lower())\n",
    "print(python_lines.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* import os\n",
    "* os.environ\n",
    "* Check environment variables are set properly. SPARK_HOME,PYHTON_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spark is a generic engine for processing large amounts of data.\n",
    "\n",
    "* Spark is written in Scala and runs on the JVM. Spark has built-in components for processing streaming data, machine learning, graph processing, and even interacting with data via SQL.\n",
    "\n",
    "* Spark is implemented in Scala, a language that runs on the JVM, so how can you access all that functionality via Python?\n",
    "\n",
    "* PySpark is the answer.\n",
    "\n",
    "* The current version of PySpark is 2.4.3 and works with Python 2.7, 3.3, and above.\n",
    "\n",
    "* You can think of PySpark as a Python-based wrapper on top of the Scala API. This means you have two sets of documentation to refer to:\n",
    "\n",
    "    * PySpark API documentation\n",
    "    * Spark Scala API documentation\n",
    "    \n",
    "* The PySpark API docs have examples, but often you’ll want to refer to the Scala documentation and translate the code into Python syntax for your PySpark programs. Luckily, Scala is a very readable function-based programming language.\n",
    "\n",
    "* PySpark communicates with the Spark Scala-based API via the Py4J library. Py4J isn’t specific to PySpark or Spark. Py4J allows any Python program to talk to JVM-based code.\n",
    "\n",
    "* There are two reasons that PySpark is based on the functional paradigm:\n",
    "\n",
    "    * Spark’s native language, Scala, is functional-based.\n",
    "    * Functional code is much easier to parallelize.\n",
    "* Another way to think of PySpark is a library that allows processing large amounts of data on a single machine or a cluster of machines.\n",
    "\n",
    "* In a Python context, think of PySpark has a way to handle parallel processing without the need for the threading or multiprocessing modules. All of the complicated communication and synchronization between threads, processes, and even different CPUs is handled by Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark API and Data Structures\n",
    "\n",
    "* To interact with PySpark, you create specialized data structures called Resilient Distributed Datasets (RDDs).\n",
    "\n",
    "* RDDs hide all the complexity of transforming and distributing your data automatically across multiple nodes by a scheduler if you’re running on a cluster.\n",
    "\n",
    "* To better understand PySpark’s API and data structures, recall the Hello World program mentioned previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "#Create a SparkContext using every core of the local machine\n",
    "sc= pyspark.SparkContext(\"local[*]\")\n",
    "\n",
    "# loda each line of the data file into RDD\n",
    "rdd = sc.textFile(\"E:\\SparkScala\\pyspark.txt\")\n",
    "\n",
    "print(\"Total words count :: \", rdd.count())\n",
    "print(rdd.filter(lambda line: \"python\" in line.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The entry-point of any PySpark program is a SparkContext object.  This object allows you to connect to a Spark cluster and create RDDs.\n",
    "* The local[*] string is a special string denoting that you’re using a local cluster, which is another way of saying you’re running in single-machine mode. The * tells Spark to create as many worker threads as logical cores on your machine.\n",
    "\n",
    "* Creating a SparkContext can be more involved when you’re using a cluster. To connect to a Spark cluster, you might need to handle authentication and a few other pieces of information specific to your cluster. You can set up those details similarly to the following: using SparkConf() object.\n",
    "\n",
    "        conf = pyspark.SparkConf()\n",
    "        conf.setMaster('spark://head_node:56887')\n",
    "        conf.set('spark.authenticate', True)\n",
    "        conf.set('spark.authenticate.secret', 'secret-key')\n",
    "        sc = SparkContext(conf=conf)\n",
    "\n",
    "## RDD Creation \n",
    "\n",
    "## parallelize():\n",
    "\n",
    "* You can create RDDs in a number of ways, but one common way is the PySpark parallelize() function. parallelize() can transform some Python data structures like lists and tuples into RDDs, which gives you functionality that makes them fault-tolerant and distributed.\n",
    "\n",
    "* To better understand RDDs, consider another example. The following code creates an iterator of 10,000 elements and then uses parallelize() to distribute that data into 2 partitions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_list = range(10000)\n",
    "rdd = sc.parallelize(big_list, 2)\n",
    "odds = rdd.filter(lambda x: x % 2 != 0)\n",
    "odds.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the above example , Created RDD using sc.parallelize method. Used, 10000 items with clusters as 2. \n",
    "\n",
    "* parallelize() turns that iterator into a distributed set of numbers and gives you all the capability of Spark’s infrastructure.\n",
    "\n",
    "* Notice that this code uses the RDD’s filter() method instead of Python’s built-in filter(), which you saw earlier. The result is the same, but what’s happening behind the scenes is drastically different. By using the RDD filter() method, that operation occurs in a distributed manner across several CPUs or computers.\n",
    "\n",
    "* Again, imagine this as Spark doing the multiprocessing work for you, all encapsulated in the RDD data structure.\n",
    "\n",
    "* take() is a way to see the contents of your RDD, but only a small subset. take() pulls that subset of data from the distributed system onto a single machine\n",
    "\n",
    "* take() is important for debugging because inspecting your entire dataset on a single machine may not be possible. RDDs are optimized to be used on Big Data so in a real world scenario a single machine may not have enough RAM to hold your entire dataset.\n",
    "\n",
    "* Note: Spark temporarily prints information to stdout when running examples like this in the shell, which you’ll see how to do soon. Your stdout might temporarily show something like [Stage 0:> (0 + 1) / 1].\n",
    "\n",
    "* The stdout text demonstrates how Spark is splitting up the RDDs and processing your data into multiple stages across different CPUs and machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TextFile:\n",
    "\n",
    "* Another way to create RDDs is to read in a file with textFile(), which you’ve seen in previous examples. RDDs are one of the foundational data structures for using PySpark so many of the functions in the API return RDDs.\n",
    "\n",
    "\n",
    "* One of the key distinctions between RDDs and other data structures is that processing is delayed until the result is requested. This is similar to a Python generator. Developers in the Python ecosystem typically use the term lazy evaluation to explain this behavior.\n",
    "\n",
    "\n",
    "* You can stack up multiple transformations on the same RDD without any processing happening. This functionality is possible because Spark maintains a directed acyclic graph of the transformations. The underlying graph is only activated when the final results are requested. In the previous example, no computation took place until you requested the results by calling take().\n",
    "\n",
    "\n",
    "* There are multiple ways to request the results from an RDD. You can explicitly request results to be evaluated and collected to a single cluster node by using collect() on a RDD. You can also implicitly request the results in various ways, one of which was using count() as you saw earlier.\n",
    "\n",
    "\n",
    "* Note: Be careful when using these methods because they pull the entire dataset into memory, which will not work if the dataset is too big to fit into the RAM of a single machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster\n",
    "* You can use the spark-submit command installed along with Spark to submit PySpark code to a cluster using the command line. This command takes a PySpark or Scala program and executes it on a cluster. This is likely how you’ll execute your real Big Data processing jobs.\n",
    "\n",
    "* Note: The path to these commands depends on where Spark was installed and will likely only work when using the referenced Docker container.\n",
    "\n",
    "* To run the Hello World example (or any PySpark program) with the running Docker container, first access the shell as described above. Once you’re in the container’s shell environment you can create files using the nano text editor.\n",
    "\n",
    "* To create the file in your current folder, simply launch nano with the name of the file you want to create:\n",
    "\n",
    "* $ nano hello_world.py\n",
    "\n",
    "* Finally, you can run the code through Spark with the pyspark-submit command:\n",
    "\n",
    "* $ /usr/local/spark/bin/spark-submit hello_world.py\n",
    "\n",
    "* This command results in a lot of output by default so it may be difficult to see your program’s output. You can control the log verbosity somewhat inside your PySpark program by changing the level on your SparkContext variable. To do that, put this line near the top of your script:\n",
    "\n",
    "* sc.setLogLevel('WARN')\n",
    "* This will omit some of the output of spark-submit so you can more clearly see the output of your program. However, in a real-world scenario, you’ll want to put any output into a file, database, or some other storage mechanism for easier debugging later.\n",
    "\n",
    "* Luckily, a PySpark program still has access to all of Python’s standard library, so saving your results to a file is not an issue:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "txt = sc.textFile(\"E:\\SparkScala\\pyspark.txt\")\n",
    "python_lines = txt.filter(lambda line: 'python' in line.lower())\n",
    "\n",
    "with open('results.txt', 'w') as file_obj:\n",
    "    file_obj.write(f'Number of lines: {txt.count()}\\n')\n",
    "    file_obj.write(f'Number of lines with python: {python_lines.count()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect  SparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.0-preview\n",
      "3.7\n",
      "local[*]\n",
      "None\n",
      "Lucky\n",
      "pyspark-shell\n",
      "local-1586862692744\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(sc.version) # Retrieve SparkContext version\n",
    "print(sc.pythonVer) # Retrieve Python version\n",
    "print(sc.master) # Master URL to connect to\n",
    "print(str(sc.sparkHome)) # Path where Spark is installed on worker nodes\n",
    "print(str(sc.sparkUser()))# Retrieve name of the Spark User running SparkContext\n",
    "print(sc.appName) # Return application name\n",
    "print(sc.applicationId) # Retrieve application ID\n",
    "print(sc.defaultParallelism)# Return default level of parallelism\n",
    "print(sc.defaultMinPartitions) # Default minimum number of partitions for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = (SparkConf()\n",
    ".setMaster(\"local\")\n",
    ".setAppName(\"My app\")\n",
    ".set(\"spark.executor.memory\", \"1g\"))\n",
    "\n",
    "sconfig = SparkContext(conf = conf)\n",
    "\n",
    "print(sc.appName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data :: parallelize colletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd 3\n",
      "rdd2 3\n",
      "rdd3 100\n",
      "rdd4 2\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([('a',7),('a',2),('b',2)])\n",
    "print(\"rdd\", rdd.count())\n",
    "rdd2 = sc.parallelize([('a',2),('d',1),('b',1)])\n",
    "print(\"rdd2\", rdd2.count())\n",
    "rdd3 = sc.parallelize(range(100))\n",
    "print(\"rdd3\", rdd3.count())\n",
    "rdd4 = sc.parallelize([(\"a\",[\"x\",\"y\",\"z\"]),\n",
    "(\"b\",[\"p\", \"r\"])])\n",
    "print(\"rdd4\", rdd4.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data :: External"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read either one text file from HDFS, a local file system or or any Hadoop-supported file system URI with textFile(),  or read in a directory of text files with wholeTextFiles().\n",
    "\n",
    "        sc.textFile(\"path\")\n",
    "        sc.wholeTextFiles(\"directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "defaultdict(<class 'int'>, {'a': 2, 'b': 1})\n",
      "defaultdict(<class 'int'>, {('a', 7): 1, ('a', 2): 1, ('b', 2): 1})\n",
      "{'a': 2, 'b': 2}\n",
      "4950\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(rdd.getNumPartitions()) # List the number of partitions\n",
    "print(rdd.count())   # Count RDD instances\n",
    "print(rdd.countByKey())  # Count RDD instances by key\n",
    "\n",
    "print(rdd.countByValue()) # Count RDD instances by value\n",
    "\n",
    "print(rdd.collectAsMap()) # Return (key,value) pairs as a\n",
    "\n",
    "print(rdd3.sum()) # Sum of RDD elements\n",
    "\n",
    "print(sc.parallelize([]).isEmpty())  #Check whether RDD is empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "0\n",
      "49.5\n",
      "28.86607004772212\n",
      "833.25\n",
      "([0, 33, 66, 99], [33, 33, 34])\n",
      "(count: 100, mean: 49.5, stdev: 28.86607004772212, max: 99.0, min: 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(rdd3.max()) # Maximum value of RDD elements\n",
    "print(rdd3.min()) # Minimum value of RDD elements\n",
    "\n",
    "print(rdd3.mean()) # Mean value of RDD elements\n",
    "\n",
    "print(rdd3.stdev()) # Standard deviation of RDD elements\n",
    "\n",
    "print(rdd3.variance())  # Compute variance of RDD elements\n",
    "print(rdd3.histogram(3) ) #Compute histogram by bins\n",
    "\n",
    "print(rdd3.stats()) # Summary statistics (count, mean, stdev, max &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Applying Functions\n",
    "\n",
    "rdd.map(lambda x: x+(x[1],x[0])).collect() #Apply a function to each RDD element\n",
    "rdd5 = rdd.flatMap(lambda x: x+(x[1],x[0])) #Apply a function to each RDD element and flatten the result\n",
    "rdd5.collect()\n",
    "rdd4.flatMapValues(lambda x: x).collect() #Apply a flatMap function to each (key,value) pair of rdd4 without changing the keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 7), ('a', 2), ('b', 2)]\n",
      "[('a', 7), ('a', 2)]\n",
      "('a', 7)\n",
      "[('b', 2), ('a', 7)]\n",
      "[3, 4, 26, 30, 39, 40, 41, 42, 52, 63, 76, 79, 80, 86, 97]\n",
      "[('a', 7), ('a', 2)]\n",
      "['b', 'a', 2, 7]\n",
      "['a', 'a', 'b']\n"
     ]
    }
   ],
   "source": [
    "# Getting\n",
    "print(rdd.collect()) # Return a list with all RDD elements\n",
    "\n",
    "print(rdd.take(2)) # Take first 2 RDD elements\n",
    "\n",
    "print(rdd.first()) # Take first RDD element\n",
    "\n",
    "print(rdd.top(2)) # Take top 2 RDD elements\n",
    "\n",
    "## Sampling\n",
    "print(rdd3.sample(False, 0.15, 81).collect()) # Return sampled subset of rdd3\n",
    "\n",
    "## Filtering\n",
    "print(rdd.filter(lambda x: \"a\" in x).collect()) # Filter the RDD\n",
    "\n",
    "\n",
    "print(rdd5.distinct().collect()) # Return distinct RDD values\n",
    "\n",
    "print(rdd.keys().collect()) # Return (key,value) RDD's keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iterating\n",
    "\n",
    "def g(x): print(x)\n",
    "\n",
    "rdd.foreach(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reducing\n",
    "rdd.reduceByKey(lambda x,y : x+y).collect() # Merge the rdd values for each key\n",
    "\n",
    "rdd.reduce(lambda a, b: a + b)  # Merge the rdd values\n",
    "\n",
    "## Grouping by\n",
    "rdd3.groupBy(lambda x: x % 2).mapValues(list).collect()      # Return RDD of grouped values\n",
    "\n",
    " \n",
    "rdd.groupByKey().mapValues(list).collect()           # Group rdd by key\n",
    "\n",
    "##Aggregating\n",
    "\n",
    "seqOp = (lambda x,y: (x[0]+y,x[1]+1))\n",
    "combOp = (lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
    "rdd3.aggregate((0,0),seqOp,combOp)            #Aggregate RDD elements of each\n",
    "\n",
    "rdd.aggregateByKey((0,0),seqOp,combOp).collect()        # Aggregate values of each RDD key\n",
    "\n",
    "rdd3.fold(0,'add').collect()                              #Aggregate the elements of each 4950 partition, and then the results\n",
    "\n",
    "rdd.foldByKey(0, 'add').collect()                         # Merge the values for each key\n",
    "\n",
    "rdd3.keyBy(lambda x: x+x).collect()                        #Create tuples of RDD elements by applying a function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2.sortBy(lambda x: x[1]).collect()    # Sort RDD by given function\n",
    "rdd2.sortByKey().collect()               # Sort (key, value) RDD by key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.repartition(4)                #New RDD with 4 partitions\n",
    "rdd.coalesce(1)               # descreses partiion to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save RDD\n",
    "rdd.saveAsTextFile(\"rdd.txt\")\n",
    "\n",
    "rdd.saveAsHadoopFile(\"hdfs://namenodehost/parent/child\",'org.apache.hadoop.mapred.TextOutputFormat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stop SparkContext\n",
    "sc.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
